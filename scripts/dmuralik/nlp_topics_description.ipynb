{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**https://www.machinelearningplus.com/nlp/lemmatization-examples-python/**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dineshmurali/miniconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2723: DtypeWarning: Columns (18,19,20,21,22,23,43,44,46,48,53,57,61,62,64,65,67,68,69,74,76,77,78,86,87,89,98) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../data/brownfields_data_with_county_geoid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The subject property consists of approximately...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Historic land use of the Bridgepoint Business ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>52 vacant lots in the City of St. Louis that w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>52 vacant lots in the City of St. Louis that w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>52 vacant lots in the City of St. Louis that w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Description\n",
       "0  The subject property consists of approximately...\n",
       "1  Historic land use of the Bridgepoint Business ...\n",
       "2  52 vacant lots in the City of St. Louis that w...\n",
       "3  52 vacant lots in the City of St. Louis that w...\n",
       "4  52 vacant lots in the City of St. Louis that w..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study_fields = ['Description/History']\n",
    "study_data = df[study_fields].rename(columns = {'Description/History' : 'Description'})\n",
    "study_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "study_data = study_data.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dineshmurali/miniconda3/lib/python3.5/site-packages/sklearn/utils/fixes.py:313: FutureWarning: numpy not_equal will not check object identity in the future. The comparison did not return the same result as suggested by the identity (`is`)) and will change.\n",
      "  _nan_object_mask = _nan_object_array != _nan_object_array\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/dineshmurali/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "cachedStopWords = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/dineshmurali/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/dineshmurali/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0 in /Users/dineshmurali/miniconda3/lib/python3.5/site-packages (2.0.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2; however, version 20.3.4 is available.\n",
      "You should consider upgrading via the '/Users/dineshmurali/miniconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "\n",
      "\u001b[93m    Linking successful\u001b[0m\n",
      "    /Users/dineshmurali/miniconda3/lib/python3.5/site-packages/en_core_web_sm\n",
      "    -->\n",
      "    /Users/dineshmurali/miniconda3/lib/python3.5/site-packages/spacy/data/en\n",
      "\n",
      "    You can now load the model via spacy.load('en')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the strip bat be hang on -PRON- foot for good'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "from bs4 import BeautifulSoup\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component needed for lemmatization\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "sentence = \"The striped bats are hanging on their feet for best\"\n",
    "\n",
    "# Parse the sentence using the loaded 'en' model object `nlp`\n",
    "doc = nlp(sentence)\n",
    "\n",
    "# Extract the lemma for each token and join\n",
    "\" \".join([token.lemma_ for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# My list of stop words. these were generated based on few iterations of the model\n",
    "stop_list = ['site', 'property', 'use', 'approximately', 'building', 'build', 'inc', 'llc', 'mary']\n",
    "\n",
    "# Updates spaCy's default stop words list with my additional words. \n",
    "nlp.Defaults.stop_words.update(stop_list)\n",
    "\n",
    "def spacyLemmatize(text):\n",
    "    #1 remove html tags\n",
    "    # Initialize the BeautifulSoup object to strip off html tags     \n",
    "    textNoHtml = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    #2 remove numbers and punctuation\n",
    "    # Use regular expressions to do a find-and-replace\n",
    "    lettersOnly = re.sub(\"[^a-zA-Z]\",\" \",textNoHtml)\n",
    "    # 3. Convert to lower case, split into individual words\n",
    "    words = lettersOnly.lower().split()\n",
    "    #3 remove stop words\n",
    "    # In Python, searching a set is much faster than searching\n",
    "    #   a list, so convert the stop words to a set\n",
    "    stops = set(cachedStopWords)\n",
    "    woStopWords = [word for word in words if not word in stops]\n",
    "\n",
    "    doc = nlp(\" \".join(woStopWords))\n",
    "    \n",
    "    # Pronouns (like \"I\" and \"you\" get lemmatized to '-PRON-', so I'm removing those.\n",
    "    return ([token.lemma_ for token in doc if token.lemma_ != '-PRON-'])\n",
    "    \n",
    "\n",
    "\n",
    "lemmatized = study_data['Description'].apply(spacyLemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [subject, property, consist, approximately, ac...\n",
       "1     [historic, land, use, bridgepoint, business, p...\n",
       "2     [vacant, lot, city, st, louis, develop, reside...\n",
       "3     [vacant, lot, city, st, louis, develop, reside...\n",
       "4     [vacant, lot, city, st, louis, develop, reside...\n",
       "5     [vacant, lot, city, st, louis, develop, reside...\n",
       "6     [vacant, lot, city, st, louis, develop, reside...\n",
       "7     [vacant, lot, city, st, louis, develop, reside...\n",
       "8     [vacant, lot, city, st, louis, develop, reside...\n",
       "9     [vacant, lot, city, st, louis, develop, reside...\n",
       "10    [vacant, lot, city, st, louis, develop, reside...\n",
       "11    [vacant, lot, city, st, louis, develop, reside...\n",
       "12    [vacant, lot, city, st, louis, develop, reside...\n",
       "13    [acre, parcel, contain, two, vacant, building,...\n",
       "14    [acre, parcel, contain, two, vacant, building,...\n",
       "15    [past, us, include, commerical, use, parking, ...\n",
       "16    [property, always, use, commercially, historic...\n",
       "17    [acre, property, formerly, use, store, tractor...\n",
       "18    [property, always, use, commercially, historic...\n",
       "19    [historic, land, use, bridgepoint, business, p...\n",
       "Name: Description, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Creates, which is a mapping of word IDs to words.\n",
    "words = corpora.Dictionary(lemmatized)\n",
    "\n",
    "# Turns each document into a bag of words.\n",
    "corpus = [words.doc2bow(doc) for doc in lemmatized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=words,\n",
    "                                           num_topics=10, \n",
    "                                           random_state=2,\n",
    "                                           update_every=1,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.118*\"site\" + 0.069*\"use\" + 0.032*\"former\" + 0.029*\"station\" + '\n",
      "  '0.025*\"currently\" + 0.025*\"operate\" + 0.021*\"city\" + 0.021*\"own\" + '\n",
      "  '0.019*\"vacant\" + 0.018*\"owner\"'),\n",
      " (1,\n",
      "  '0.064*\"site\" + 0.030*\"tank\" + 0.027*\"ust\" + 0.026*\"storage\" + 0.026*\"oil\" + '\n",
      "  '0.018*\"remove\" + 0.015*\"petroleum\" + 0.014*\"material\" + 0.013*\"waste\" + '\n",
      "  '0.013*\"underground\"'),\n",
      " (2,\n",
      "  '0.121*\"property\" + 0.024*\"parcel\" + 0.024*\"portion\" + 0.023*\"subject\" + '\n",
      "  '0.021*\"develop\" + 0.021*\"residential\" + 0.019*\"land\" + 0.017*\"least\" + '\n",
      "  '0.017*\"commercial\" + 0.017*\"vacant\"'),\n",
      " (3,\n",
      "  '0.077*\"school\" + 0.033*\"store\" + 0.023*\"center\" + 0.021*\"hospital\" + '\n",
      "  '0.019*\"grocery\" + 0.017*\"community\" + 0.015*\"tract\" + 0.013*\"new\" + '\n",
      "  '0.011*\"mary\" + 0.011*\"theater\"'),\n",
      " (4,\n",
      "  '0.052*\"company\" + 0.040*\"operation\" + 0.033*\"industrial\" + 0.027*\"include\" '\n",
      "  '+ 0.026*\"facility\" + 0.022*\"railroad\" + 0.022*\"storage\" + 0.021*\"yard\" + '\n",
      "  '0.019*\"use\" + 0.019*\"manufacturing\"'),\n",
      " (5,\n",
      "  '0.106*\"shop\" + 0.077*\"repair\" + 0.071*\"auto\" + 0.044*\"automotive\" + '\n",
      "  '0.034*\"sale\" + 0.027*\"garage\" + 0.027*\"car\" + 0.022*\"service\" + '\n",
      "  '0.021*\"automobile\" + 0.015*\"tire\"'),\n",
      " (6,\n",
      "  '0.135*\"building\" + 0.037*\"build\" + 0.028*\"construct\" + 0.027*\"two\" + '\n",
      "  '0.024*\"foot\" + 0.024*\"office\" + 0.022*\"one\" + 0.020*\"square\" + '\n",
      "  '0.019*\"story\" + 0.017*\"approximately\"'),\n",
      " (7,\n",
      "  '0.035*\"city\" + 0.024*\"park\" + 0.023*\"acquire\" + 0.019*\"purchase\" + '\n",
      "  '0.017*\"county\" + 0.017*\"housing\" + 0.015*\"acre\" + 0.014*\"tax\" + '\n",
      "  '0.014*\"redevelopment\" + 0.012*\"public\"'),\n",
      " (8,\n",
      "  '0.039*\"phase\" + 0.038*\"soil\" + 0.026*\"environmental\" + 0.024*\"assessment\" + '\n",
      "  '0.021*\"esa\" + 0.020*\"groundwater\" + 0.016*\"complete\" + 0.015*\"ii\" + '\n",
      "  '0.015*\"conduct\" + 0.015*\"asbestos\"'),\n",
      " (9,\n",
      "  '0.044*\"mill\" + 0.018*\"sell\" + 0.017*\"purchase\" + 0.016*\"complex\" + '\n",
      "  '0.015*\"system\" + 0.015*\"water\" + 0.015*\"district\" + 0.012*\"inc\" + 0.011*\"n\" '\n",
      "  '+ 0.009*\"llc\"')]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "#1st iteration\n",
    "pprint(lda_model.print_topics(num_words=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.068*\"building\" + 0.044*\"use\" + 0.034*\"property\" + 0.024*\"commercial\" + '\n",
      "  '0.023*\"residential\" + 0.023*\"currently\" + 0.023*\"build\" + 0.019*\"vacant\" + '\n",
      "  '0.017*\"former\" + 0.017*\"station\"'),\n",
      " (1,\n",
      "  '0.033*\"property\" + 0.032*\"city\" + 0.020*\"phase\" + 0.016*\"purchase\" + '\n",
      "  '0.015*\"site\" + 0.015*\"county\" + 0.011*\"assessment\" + 0.010*\"esa\" + '\n",
      "  '0.010*\"community\" + 0.010*\"redevelopment\"'),\n",
      " (2,\n",
      "  '0.032*\"mr\" + 0.030*\"owner\" + 0.024*\"sell\" + 0.019*\"paper\" + 0.016*\"llc\" + '\n",
      "  '0.015*\"ownership\" + 0.012*\"substance\" + 0.012*\"purchase\" + 0.012*\"inc\" + '\n",
      "  '0.011*\"ravine\"'),\n",
      " (3,\n",
      "  '0.101*\"shop\" + 0.074*\"repair\" + 0.068*\"auto\" + 0.042*\"automotive\" + '\n",
      "  '0.031*\"sale\" + 0.031*\"service\" + 0.026*\"car\" + 0.025*\"garage\" + '\n",
      "  '0.022*\"machine\" + 0.022*\"maintenance\"'),\n",
      " (4,\n",
      "  '0.103*\"property\" + 0.055*\"site\" + 0.029*\"portion\" + 0.028*\"subject\" + '\n",
      "  '0.025*\"develop\" + 0.021*\"least\" + 0.019*\"since\" + 0.019*\"occupy\" + '\n",
      "  '0.017*\"historical\" + 0.015*\"land\"'),\n",
      " (5,\n",
      "  '0.049*\"parcel\" + 0.044*\"site\" + 0.037*\"locate\" + 0.036*\"area\" + '\n",
      "  '0.034*\"street\" + 0.024*\"acre\" + 0.024*\"north\" + 0.022*\"west\" + 0.022*\"east\" '\n",
      "  '+ 0.022*\"south\"'),\n",
      " (6,\n",
      "  '0.056*\"industrial\" + 0.045*\"company\" + 0.038*\"railroad\" + 0.035*\"yard\" + '\n",
      "  '0.034*\"include\" + 0.031*\"manufacturing\" + 0.031*\"operation\" + '\n",
      "  '0.029*\"facility\" + 0.024*\"manufacture\" + 0.023*\"metal\"'),\n",
      " (7,\n",
      "  '0.030*\"site\" + 0.028*\"tank\" + 0.025*\"ust\" + 0.024*\"oil\" + 0.022*\"soil\" + '\n",
      "  '0.017*\"material\" + 0.014*\"petroleum\" + 0.012*\"remove\" + '\n",
      "  '0.012*\"environmental\" + 0.012*\"underground\"'),\n",
      " (8,\n",
      "  '0.059*\"mill\" + 0.034*\"plant\" + 0.019*\"steel\" + 0.018*\"complex\" + '\n",
      "  '0.016*\"operation\" + 0.013*\"company\" + 0.012*\"power\" + 0.012*\"fort\" + '\n",
      "  '0.012*\"boiler\" + 0.010*\"print\"'),\n",
      " (9,\n",
      "  '0.024*\"co\" + 0.019*\"medical\" + 0.014*\"valley\" + 0.014*\"air\" + '\n",
      "  '0.014*\"chemical\" + 0.013*\"inc\" + 0.013*\"campus\" + 0.011*\"army\" + '\n",
      "  '0.010*\"facility\" + 0.010*\"tool\"')]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "#1st iteration\n",
    "pprint(lda_model.print_topics(num_words=10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
